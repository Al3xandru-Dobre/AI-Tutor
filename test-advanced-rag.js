// test-advanced-rag.js - Test script for advanced RAG features

const TransformerEmbeddingService = require('./backend/services/TransformerEmbeddingService');
const CrossEncoderService = require('./backend/services/CrossEncoderService');
const JapaneseTokenizerService = require('./backend/services/JapaneseTokenizerService');

async function testTransformerEmbeddings() {
  console.log('\nüß™ Testing Transformer Embeddings...\n');

  const service = new TransformerEmbeddingService({
    modelName: 'Xenova/paraphrase-multilingual-MiniLM-L12-v2'
  });

  await service.initialize();

  // Test Japanese-English semantic similarity
  const texts = [
    "Êó•Êú¨Ë™û„ÅÆÂä©Ë©û„Å´„Å§„ÅÑ„Å¶",
    "Japanese particles explanation",
    "How to cook sushi"
  ];

  console.log('Generating embeddings for:');
  texts.forEach((t, i) => console.log(`  ${i + 1}. ${t}`));

  const embeddings = await service.generate(texts);

  console.log('\nüìä Similarity Matrix:');
  console.log('                     ', texts.map((_, i) => `Text ${i + 1}`).join('  '));

  for (let i = 0; i < embeddings.length; i++) {
    const similarities = embeddings.map(emb =>
      service.cosineSimilarity(embeddings[i], emb).toFixed(3)
    );
    console.log(`Text ${i + 1} (${texts[i].substring(0, 20)}...)`, similarities.join('   '));
  }

  console.log('\n‚úÖ Expected: Text 1 and 2 should have high similarity (>0.6)');
  console.log('‚úÖ Expected: Text 1 and 3 should have low similarity (<0.4)');

  const stats = service.getStats();
  console.log('\nüìà Service Stats:');
  console.log(JSON.stringify(stats, null, 2));

  return service;
}

async function testJapaneseTokenization() {
  console.log('\n\nüß™ Testing Japanese Tokenization...\n');

  const tokenizer = new JapaneseTokenizerService();
  await tokenizer.initialize();

  const testTexts = [
    "ÁßÅ„ÅØÂ≠¶Áîü„Åß„Åô",
    "Êó•Êú¨Ë™û„ÇíÂãâÂº∑„Åó„Å¶„ÅÑ„Åæ„Åô",
    "ÂΩºÂ•≥„ÅØÊØéÊó•Âõ≥Êõ∏È§®„ÅßÂãâÂº∑„Åó„Å¶„ÅÑ„Çã"
  ];

  for (const text of testTexts) {
    console.log(`\nüìù Text: "${text}"`);

    // Basic tokenization
    const tokens = tokenizer.tokenize(text, { includeMetadata: false });
    console.log('   Tokens:', tokens.join(' | '));

    // Detailed tokenization
    const detailed = tokenizer.tokenize(text, { includeMetadata: true });
    console.log('   Detailed:');
    detailed.forEach(token => {
      console.log(`     - ${token.surface} (${token.pos}) [${token.baseForm}]`);
    });

    // Keywords
    const keywords = tokenizer.extractKeywords(text);
    console.log('   Keywords:', keywords.join(', '));

    // JLPT Level
    const level = tokenizer.analyzeJLPTLevel(text);
    console.log(`   JLPT Level: ${level.level} (confidence: ${(level.confidence * 100).toFixed(0)}%)`);
  }

  console.log('\nüìä Mixed Text Tokenization:');
  const mixed = "‰ªäÊó•„ÅØgood„Å™Â§©Ê∞ó„Åß„Åô";
  const mixedTokens = tokenizer.tokenizeMixed(mixed);
  console.log(`   Text: "${mixed}"`);
  console.log(`   Tokens:`, mixedTokens);

  const stats = tokenizer.getStats();
  console.log('\nüìà Tokenizer Stats:');
  console.log(JSON.stringify(stats, null, 2));

  return tokenizer;
}

async function testCrossEncoderReranking() {
  console.log('\n\nüß™ Testing Cross-Encoder Reranking...\n');

  const crossEncoder = new CrossEncoderService();
  await crossEncoder.initialize();

  const query = "How to use the „ÅØ particle in Japanese";

  const results = [
    {
      content: "The „ÅØ particle is used as a topic marker in Japanese. It indicates what the sentence is about.",
      score: 0.65,
      id: 1
    },
    {
      content: "Japanese cuisine includes sushi, ramen, and tempura. These dishes are popular worldwide.",
      score: 0.70,
      id: 2
    },
    {
      content: "The difference between „ÅØ and „Åå particles is subtle. „ÅØ marks the topic, while „Åå marks the subject.",
      score: 0.68,
      id: 3
    },
    {
      content: "Tokyo is the capital of Japan. It has many museums and cultural sites.",
      score: 0.60,
      id: 4
    }
  ];

  console.log(`Query: "${query}"`);
  console.log('\nüìã Original Results (sorted by score):');
  results.sort((a, b) => b.score - a.score);
  results.forEach((r, i) => {
    console.log(`  ${i + 1}. [ID:${r.id}] Score: ${r.score.toFixed(3)}`);
    console.log(`     ${r.content.substring(0, 70)}...`);
  });

  // Rerank with cross-encoder
  console.log('\nüîÑ Reranking with Cross-Encoder...');
  const reranked = await crossEncoder.rerank(query, results, {
    topK: 4,
    textField: 'content'
  });

  console.log('\nüéØ Reranked Results:');
  reranked.forEach((r, i) => {
    console.log(`  ${i + 1}. [ID:${r.id}] Original: ${r.original_score.toFixed(3)}, Relevance: ${r.relevance_score.toFixed(3)}`);
    console.log(`     ${r.content.substring(0, 70)}...`);
  });

  console.log('\n‚úÖ Expected: Particle-related docs (ID 1, 3) should rank higher');
  console.log('‚úÖ Expected: Irrelevant docs (ID 2, 4) should rank lower');

  // Test hybrid reranking
  console.log('\nüîÑ Hybrid Reranking (70% cross-encoder + 30% original)...');
  const hybrid = await crossEncoder.rerankHybrid(query, results, {
    topK: 4,
    crossEncoderWeight: 0.7
  });

  console.log('\nüéØ Hybrid Results:');
  hybrid.forEach((r, i) => {
    console.log(`  ${i + 1}. [ID:${r.id}] Hybrid: ${r.hybrid_score.toFixed(3)}`);
    console.log(`     Original: ${r.original_score.toFixed(3)}, Relevance: ${r.relevance_score.toFixed(3)}`);
  });

  const stats = crossEncoder.getStats();
  console.log('\nüìà Cross-Encoder Stats:');
  console.log(JSON.stringify(stats, null, 2));

  return crossEncoder;
}

async function runAllTests() {
  console.log('='.repeat(80));
  console.log('üöÄ Advanced RAG Implementation Tests');
  console.log('='.repeat(80));

  try {
    // Test 1: Transformer Embeddings
    const embeddingService = await testTransformerEmbeddings();
    await embeddingService.cleanup();

    // Test 2: Japanese Tokenization
    const tokenizer = await testJapaneseTokenization();
    await tokenizer.cleanup();

    // Test 3: Cross-Encoder Reranking
    const crossEncoder = await testCrossEncoderReranking();
    await crossEncoder.cleanup();

    console.log('\n' + '='.repeat(80));
    console.log('‚úÖ All tests completed successfully!');
    console.log('='.repeat(80));

    console.log('\nüìù Summary:');
    console.log('  ‚úì Real transformer embeddings working');
    console.log('  ‚úì Japanese tokenization with Kuromoji working');
    console.log('  ‚úì Cross-encoder reranking working');
    console.log('\nüéâ Your advanced RAG system is ready to use!');

  } catch (error) {
    console.error('\n‚ùå Test failed:', error);
    console.error('\nStack trace:', error.stack);
    process.exit(1);
  }
}

// Run tests if this script is executed directly
if (require.main === module) {
  runAllTests();
}

module.exports = { runAllTests };
