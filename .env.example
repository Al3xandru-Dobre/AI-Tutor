# Ollama Configuration
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3:8b
# Note: Any multilingual Open Weight Model will work

# Google Custom Search API (Optional)
GOOGLE_API_KEY=your_google_api_key_here
GOOGLE_SEARCH_ENGINE_ID=your_search_engine_id_here

# ChromaDB Configuration
USE_CHROMADB=true
CHROMA_DB_URL=http://localhost:8000
CHROMA_COLLECTION_NAME=japanese_tutor_knowledge
EMBEDDING_MODEL=all-MiniLM-L6-v2
MAX_CHUNK_SIZE=800
CHUNK_OVERLAP=100

# Model Provider Configuration
DEFAULT_MODEL_PROVIDER=ollama

# Cerebras API Configuration (Optional - for ultra-fast cloud inference)
CEREBRAS_API_KEY=your_cerebras_api_key_here
CEREBRAS_MODEL=llama-3.3-70b

# Groq API Configuration (Optional - for LPU-powered inference)
GROQ_API_KEY=your_groq_api_key_here
GROQ_MODEL=llama-3.3-70b-versatile

# Mistral AI API Configuration (Optional - for efficient multilingual models)
MISTRAL_API_KEY=your_mistral_api_key_here
MISTRAL_MODEL=mistral-large-latest

# OpenRouter API Configuration (Optional - unified API for multiple providers)
OPENROUTER_API_KEY=your_openrouter_api_key_here
OPENROUTER_MODEL=meta-llama/llama-3.3-70b-instruct

# Server Configuration
PORT=3000
NODE_ENV=development