const tf = require('@tensorflow/tfjs-node');
const { env, pipeline } = require('@xenova/transformers');
const path = require('path');

// Note: Global transformer configuration is set in backend/config/transformers.config.js
// Configure Hugging Face authentication if provided
if (process.env.HUGGING_FACE_HUB_TOKEN) {
  env.accessToken = process.env.HUGGING_FACE_HUB_TOKEN;
}
class CustomJapaneseEmbedding {
    constructor() {
        // Numele modelului de pe Hugging Face. Acesta este singurul lucru de care avem nevoie.
        // Folosim un model multilingv optimizat pentru Transformers.js (format ONNX)
        // OpÈ›iuni compatibile:
        // - 'Xenova/paraphrase-multilingual-mpnet-base-v2' (implicit)
        // - 'Xenova/multilingual-e5-base' (mai nou, performanÈ›Äƒ Ã®mbunÄƒtÄƒÈ›itÄƒ)
        // - 'Xenova/multilingual-e5-large' (cel mai performant, dar mai lent)
        this.modelName = 'Xenova/multilingual-e5-base';

        // `this.pipeline` va stoca funcÈ›ia de nivel Ã®nalt care se ocupÄƒ de tot.
        // `this.model` È™i `this.tokenizer` nu mai sunt necesare.
        this.pipeline = null;
    }

    /**
     * ÃncarcÄƒ modelul È™i tokenizer-ul Ã®ntr-un singur pipeline funcÈ›ional.
     * La prima rulare, descarcÄƒ modelul; ulterior, Ã®l Ã®ncarcÄƒ din cache-ul local.
     */
    async initialize() {
        console.log(`ğŸ”„ Se iniÈ›ializeazÄƒ serviciul de embedding pentru limba japonezÄƒ...`);
        
        try {
            console.log(`Se Ã®ncarcÄƒ pipeline-ul pentru "${this.modelName}" (model + tokenizer)...`);

            // Acesta este singurul apel necesar. El pregÄƒteÈ™te totul.
            this.pipeline = await pipeline('feature-extraction', this.modelName, {
                progress_callback: (progress) => {
                    if (progress.status === 'downloading') {
                        console.log(`Se descarcÄƒ ${progress.file}: ${Math.round(progress.progress)}%`);
                    }
                }
            });

            console.log("âœ… Serviciul de embedding pentru limba japonezÄƒ a fost iniÈ›ializat cu succes!");

        } catch (error) {
            console.error('âŒ A eÈ™uat iniÈ›ializarea serviciului de embedding:', error.message);
            throw error;
        }
        
    }

    /**
     * GenereazÄƒ embedding-uri pentru unul sau mai multe texte.
     * @param {string|string[]} texts - Un text sau un array de texte.
     * @returns {Promise<number[][]>} Un array de vectori de embedding.
     */
    async embed(texts) {
        if (!this.pipeline) {
            throw new Error("Serviciul nu este initializat. ApeleazÄƒ initialize() Ã®nainte de a genera embedding-uri.");
        }

        // AsigurÄƒm cÄƒ `texts` este un array pentru a gestiona uniform cazurile.
        const textsArray = Array.isArray(texts) ? texts : [texts];

        try {
            // ApelÄƒm direct pipeline-ul cu textul brut.
            // El se ocupÄƒ intern de tokenizare, padding, trunchiere È™i predicÈ›ie.
            const embeddingsTensor = await this.pipeline(textsArray, {
                pooling: 'mean',    // CreeazÄƒ un singur vector per text (media embedding-urilor token-urilor).
                normalize: true     // NormalizeazÄƒ vectorul, util pentru similaritate cosinus.
            });

            // Convertim tensorul rezultat Ã®ntr-un array JavaScript standard.
            return embeddingsTensor.tolist();

        } catch (error) {
            console.error('âŒ A eÈ™uat generarea embedding-urilor:', error.message);
            throw error;
        }
    }
}

module.exports = CustomJapaneseEmbedding;